CS229 Lecture Notes

Andrew Ng

updated by Tengyu Ma on April 21, 2019

Part V
Kernel Methods

1.1 Feature maps

Recall that in our discussion about linear regression, we considered the prob-
lem of predicting the price of a house (denoted by y) from the living area of
the house (denoted by x), and we ﬁt a linear function of x to the training
data. What if the price y can be more accurately represented as a non-linear
function of x? In this case, we need a more expressive family of models than
linear models.

We start by considering ﬁtting cubic functions y = θ3x3 + θ2x2 + θ1x + θ0.
It turns out that we can view the cubic function as a linear function over
the a diﬀerent set of feature variables (deﬁned below). Concretely, let the
function φ : R → R4 be deﬁned as

φ(x) =













1
x
x2
x3

∈ R4.

(1)

Let θ ∈ R4 be the vector containing θ0, θ1, θ2, θ3 as entries. Then we can

rewrite the cubic function in x as:

θ3x3 + θ2x2 + θ1x + θ0 = θT φ(x)

Thus, a cubic function of the variable x can be viewed as a linear function
over the variables φ(x). To distinguish between these two sets of variables,

1

2

in the context of kernel methods, we will call the “original” input value the
input attributes of a problem (in this case, x, the living area). When the
original input is mapped to some new set of quantities φ(x), we will call those
new quantities the features variables. (Unfortunately, diﬀerent authors use
diﬀerent terms to describe these two things in diﬀerent contexts.) We will
call φ a feature map, which maps the attributes to the features.

1.2 LMS (least mean squares) with features

We will derive the gradient descent algorithm for ﬁtting the model θT φ(x).
First recall that for ordinary least square problem where we were to ﬁt θT x,
the batch gradient descent update is (see the ﬁrst lecture note for its deriva-
tion):

θ := θ + α

:= θ + α

n
(cid:88)

i=1
n
(cid:88)

i=1

(cid:0)y(i) − hθ(x(i))(cid:1) x(i)

(cid:0)y(i) − θT x(i)(cid:1) x(i).

(2)

Let φ : Rd → Rp be a feature map that maps attribute x (in Rd) to the
features φ(x) in Rp. (In the motivating example in the previous subsection,
we have d = 1 and p = 4.) Now our goal is to ﬁt the function θT φ(x), with
θ being a vector in Rp instead of Rd. We can replace all the occurrences of
x(i) in the algorithm above by φ(x(i)) to obtain the new update:

θ := θ + α

n
(cid:88)

i=1

(cid:0)y(i) − θT φ(x(i))(cid:1) φ(x(i))

Similarly, the corresponding stochastic gradient descent update rule is

θ := θ + α (cid:0)y(i) − θT φ(x(i))(cid:1) φ(x(i))

(3)

(4)

1.3 LMS with the kernel trick

The gradient descent update, or stochastic gradient update above becomes
computationally expensive when the features φ(x) is high-dimensional. For
example, consider the direct extension of the feature map in equation (1) to

high-dimensional input x: suppose x ∈ Rd, and let φ(x) be the vector that
contains all the monomials of x with degree ≤ 3

3

φ(x) =




























1
x1
x2
...
x2
1
x1x2
x1x3
...
x2x1
...
x3
1
x2
1x2
...




























.

(5)

The dimension of the features φ(x) is on the order of d3.1 This is a pro-
hibitively long vector for computational purpose — when d = 1000, each
update requires at least computing and storing a 10003 = 109 dimensional
vector, which is 106 times slower than the update rule for for ordinary least
squares updates (2).

It may appear at ﬁrst that such d3 runtime per update and memory usage
are inevitable, because the vector θ itself is of dimension p ≈ d3, and we may
need to update every entry of θ and store it. However, we will introduce the
kernel trick with which we will not need to store θ explicitly, and the runtime
can be signiﬁcantly improved.

For simplicity, we assume the initialize the value θ = 0, and we focus
on the iterative update (3). The main observation is that at any time, θ
can be represented as a linear combination of the vectors φ(x(1)), . . . , φ(x(n)).
Indeed, we can show this inductively as follows. At initialization, θ = 0 =
(cid:80)n

i=1 0 · φ(x(i)). Assume at some point, θ can be represented as

θ =

n
(cid:88)

i=1

βiφ(x(i))

(6)

1Here, for simplicity, we include all the monomials with repetitions (so that, e.g., x1x2x3
and x2x3x1 both appear in φ(x)). Therefore, there are totally 1 + d + d2 + d3 entries in
φ(x).

4

for some β1, . . . , βn ∈ R. Then we claim that in the next round, θ is still a
linear combination of φ(x(1)), . . . , φ(x(n)) because

θ := θ + α

n
(cid:88)

i=1

(cid:0)y(i) − θT φ(x(i))(cid:1) φ(x(i))

=

=

n
(cid:88)

i=1
n
(cid:88)

i=1

βiφ(x(i)) + α

n
(cid:88)

i=1

(cid:0)y(i) − θT φ(x(i))(cid:1) φ(x(i))

(βi + α (cid:0)y(i) − θT φ(x(i))(cid:1))
(cid:123)(cid:122)
(cid:125)
(cid:124)
new βi

φ(x(i))

(7)

You may realize that our general strategy is to implicitly represent the p-
dimensional vector θ by a set of coeﬃcients β1, . . . , βn. Towards doing this,
we derive the update rule of the coeﬃcients β1, . . . , βn. Using the equation
above, we see that the new βi depends on the old one via

βi := βi + α (cid:0)y(i) − θT φ(x(i))(cid:1)

(8)

Here we still have the old θ on the RHS of the equation. Replacing θ by
θ = (cid:80)n

j=1 βjφ(x(j)) gives

∀i ∈ {1, . . . , n}, βi := βi + α

y(i) −

(cid:32)

βjφ(x(j))

T

φ(x(i))

(cid:33)

n
(cid:88)

j=1

We often rewrite φ(x(j))T φ(x(i)) as (cid:104)φ(x(j)), φ(x(i))(cid:105) to emphasize that it’s the
inner product of the two feature vectors. Viewing βi’s as the new representa-
tion of θ, we have successfully translated the batch gradient descent algorithm
into an algorithm that updates the value of β iteratively. It may appear that
at every iteration, we still need to compute the values of (cid:104)φ(x(j)), φ(x(i))(cid:105) for
all pairs of i, j, each of which may take roughly O(p) operation. However,
two important properties come to rescue:

1. We can pre-compute the pairwise inner products (cid:104)φ(x(j)), φ(x(i))(cid:105) for all

pairs of i, j before the loop starts.

2. For the feature map φ deﬁned in (5) (or many other interesting fea-
ture maps), computing (cid:104)φ(x(j)), φ(x(i))(cid:105) can be eﬃcient and does not

necessarily require computing φ(x(i)) explicitly. This is because:

5

(cid:104)φ(x), φ(z)(cid:105) = 1 +

= 1 +

d
(cid:88)

i=1

d
(cid:88)

i=1

xizi +

(cid:88)

xixjzizj +

(cid:88)

xixjxkzizjzk

i,j∈{1,...,d}
(cid:32) d

(cid:88)

(cid:33)2

i,j,k∈{1,...,d}
(cid:33)3

(cid:32) d

(cid:88)

xizi +

xizi

+

xizi

i=1

i=1

= 1 + (cid:104)x, z(cid:105) + (cid:104)x, z(cid:105)2 + (cid:104)x, z(cid:105)3

(9)

Therefore, to compute (cid:104)φ(x), φ(z)(cid:105), we can ﬁrst compute (cid:104)x, z(cid:105) with
O(d) time and then take another constant number of operations to com-
pute 1 + (cid:104)x, z(cid:105) + (cid:104)x, z(cid:105)2 + (cid:104)x, z(cid:105)3.

As you will see, the inner products between the features (cid:104)φ(x), φ(z)(cid:105) are
essential here. We deﬁne the Kernel corresponding to the feature map φ as
a function that maps X × X → R satisfying: 2

K(x, z) (cid:44) (cid:104)φ(x), φ(z)(cid:105)

(10)

To wrap up the discussion, we write the down the ﬁnal algorithm as

follows:

1. Compute all the values K(x(i), x(j)) (cid:44) (cid:104)φ(x(i)), φ(x(j))(cid:105) using equa-

tion (9) for all i, j ∈ {1, . . . , n}. Set β := 0.

2. Loop:

∀i ∈ {1, . . . , n}, βi := βi + α

y(i) −

(cid:32)

(cid:33)

βjK(x(i), x(j))

(11)

n
(cid:88)

j=1

Or in vector notation, letting K be the n × n matrix with Kij =

K(x(i), x(j)), we have

β := β + α((cid:126)y − Kβ)

With the algorithm above, we can update the representation β of the
vector θ eﬃciently with O(n) time per update. Finally, we need to show that

2Recall that X is the space of the input x. In our running example, X = Rd

6

the knowledge of the representation β suﬃces to compute the prediction
θT φ(x). Indeed, we have

θT φ(x) =

n
(cid:88)

i=1

T
βiφ(x(i))

φ(x) =

n
(cid:88)

i=1

βiK(x(i), x)

(12)

You may realize that fundamentally all we need to know about the feature
map φ(·) is encapsulated in the corresponding kernel function K(·, ·). We
will expand on this in the next section.

1.4 Properties of kernels

In the last subsection, we started with an explicitly deﬁned feature map φ,
which induces the kernel function K(x, z) (cid:44) (cid:104)φ(x), φ(z)(cid:105). Then we saw that
the kernel function is so intrinsic so that as long as the kernel function is
deﬁned, the whole training algorithm can be written entirely in the language
of the kernel without referring to the feature map φ, so can the prediction of
a test example x (equation (12).)

Therefore, it would be tempted to deﬁne other kernel function K(·, ·)
and run the algorithm (11). Note that the algorithm (11) does not need to
explicitly access the feature map φ, and therefore we only need to ensure the
existence of the feature map φ, but do not necessarily need to be able to
explicitly write φ down.

What kinds of functions K(·, ·) can correspond to some feature map φ? In
other words, can we tell if there is some feature mapping φ so that K(x, z) =
φ(x)T φ(z) for all x, z?

If we can answer this question by giving a precise characterization of valid
kernel functions, then we can completely change the interface of selecting
feature maps φ to the interface of selecting kernel function K. Concretely,
we can pick a function K, verify that it satisﬁes the characterization (so
that there exists a feature map φ that K corresponds to), and then we can
run update rule (11). The beneﬁt here is that we don’t have to be able
to compute φ or write it down analytically, and we only need to know its
existence. We will answer this question at the end of this subsection after
we go through several concrete examples of kernels.

Suppose x, z ∈ Rd, and let’s ﬁrst consider the function K(·, ·) deﬁned as:

K(x, z) = (xT z)2.

7

We can also write this as

K(x, z) =

(cid:32) d

(cid:88)

(cid:33) (cid:32) d

(cid:88)

(cid:33)

xjzj

xizi

i=1

j=1

=

=

d
(cid:88)

d
(cid:88)

i=1

j=1

xixjzizj

d
(cid:88)

(xixj)(zizj)

i,j=1

Thus, we see that K(x, z) = (cid:104)φ(x), φ(z)(cid:105) is the kernel function that corre-
sponds to the the feature mapping φ given (shown here for the case of d = 3)
by



φ(x) =

















.















x1x1
x1x2
x1x3
x2x1
x2x2
x2x3
x3x1
x3x2
x3x3

Revisiting the computational eﬃciency perspective of kernel, note that whereas
calculating the high-dimensional φ(x) requires O(d2) time, ﬁnding K(x, z)
takes only O(d) time—linear in the dimension of the input attributes.

For another related example, also consider K(·, ·) deﬁned by

K(x, z) = (xT z + c)2

d
(cid:88)

=

(xixj)(zizj) +

i,j=1

d
(cid:88)

√
(

i=1

√

2cxi)(

2czi) + c2.

(Check this yourself.) This function K is a kernel function that corresponds

to the feature mapping (again shown for d = 3)

8

φ(x) =

























x1x1
x1x2
x1x3
x2x1
x2x2
x2x3
x3x1
x3x2
x3x3√
2cx1√
2cx2√
2cx3
c

























,

and the parameter c controls the relative weighting between the xi (ﬁrst
order) and the xixj (second order) terms.

k

More broadly, the kernel K(x, z) = (xT z + c)k corresponds to a feature
mapping to an (cid:0)d+k
(cid:1) feature space, corresponding of all monomials of the
form xi1xi2 . . . xik that are up to order k. However, despite working in this
O(dk)-dimensional space, computing K(x, z) still takes only O(d) time, and
hence we never need to explicitly represent feature vectors in this very high
dimensional feature space.

Kernels as similarity metrics. Now, let’s talk about a slightly diﬀerent
view of kernels. Intuitively, (and there are things wrong with this intuition,
but nevermind), if φ(x) and φ(z) are close together, then we might expect
K(x, z) = φ(x)T φ(z) to be large. Conversely, if φ(x) and φ(z) are far apart—
say nearly orthogonal to each other—then K(x, z) = φ(x)T φ(z) will be small.
So, we can think of K(x, z) as some measurement of how similar are φ(x)
and φ(z), or of how similar are x and z.

Given this intuition, suppose that for some learning problem that you’re
working on, you’ve come up with some function K(x, z) that you think might
be a reasonable measure of how similar x and z are. For instance, perhaps
you chose

K(x, z) = exp

−

(cid:18)

||x − z||2
2σ2

(cid:19)

.

This is a reasonable measure of x and z’s similarity, and is close to 1 when
x and z are close, and near 0 when x and z are far apart. Does there exist

9

a feature map φ such that the kernel K deﬁned above satisﬁes K(x, z) =
φ(x)T φ(z)? In this particular example, the answer is yes. This kernel is called
the Gaussian kernel, and corresponds to an inﬁnite dimensional feature
mapping φ. We will give a precise characterization about what properties
a function K needs to satisfy so that it can be a valid kernel function that
corresponds to some feature map φ.

Necessary conditions for valid kernels. Suppose for now that K is
indeed a valid kernel corresponding to some feature mapping φ, and we will
ﬁrst see what properties it satisﬁes. Now, consider some ﬁnite set of n points
(not necessarily the training set) {x(1), . . . , x(n)}, and let a square, n-by-n
matrix K be deﬁned so that its (i, j)-entry is given by Kij = K(x(i), x(j)).
This matrix is called the kernel matrix. Note that we’ve overloaded the
notation and used K to denote both the kernel function K(x, z) and the
kernel matrix K, due to their obvious close relationship.

Now, if K is a valid kernel, then Kij = K(x(i), x(j)) = φ(x(i))T φ(x(j)) =
φ(x(j))T φ(x(i)) = K(x(j), x(i)) = Kji, and hence K must be symmetric. More-
over, letting φk(x) denote the k-th coordinate of the vector φ(x), we ﬁnd that
for any vector z, we have

zT Kz =

=

=

=

=

(cid:88)

(cid:88)

i
(cid:88)

j
(cid:88)

i
(cid:88)

j
(cid:88)

ziKijzj

ziφ(x(i))T φ(x(j))zj

(cid:88)

zi

φk(x(i))φk(x(j))zj

i
(cid:88)

j
(cid:88)

k
(cid:88)
ziφk(x(i))φk(x(j))zj

j

i
(cid:32)

k

(cid:88)

(cid:88)

(cid:33)2

ziφk(x(i))

k
≥ 0.

i

The second-to-last step uses the fact that (cid:80)
i ai)2 for ai =
ziφk(x(i)). Since z was arbitrary, this shows that K is positive semi-deﬁnite
(K ≥ 0).

i,j aiaj = ((cid:80)

Hence, we’ve shown that if K is a valid kernel (i.e., if it corresponds to
some feature mapping φ), then the corresponding kernel matrix K ∈ Rn×n
is symmetric positive semideﬁnite.

10

Suﬃcient conditions for valid kernels. More generally, the condition
above turns out to be not only a necessary, but also a suﬃcient, condition
for K to be a valid kernel (also called a Mercer kernel). The following result
is due to Mercer.3

Theorem (Mercer). Let K : Rd × Rd (cid:55)→ R be given. Then for K
to be a valid (Mercer) kernel, it is necessary and suﬃcient that for any
{x(1), . . . , x(n)}, (n < ∞), the corresponding kernel matrix is symmetric pos-
itive semi-deﬁnite.

Given a function K, apart from trying to ﬁnd a feature mapping φ that
corresponds to it, this theorem therefore gives another way of testing if it is
a valid kernel. You’ll also have a chance to play with these ideas more in
problem set 2.

In class, we also brieﬂy talked about a couple of other examples of ker-
nels. For instance, consider the digit recognition problem, in which given
an image (16x16 pixels) of a handwritten digit (0-9), we have to ﬁgure out
which digit it was. Using either a simple polynomial kernel K(x, z) = (xT z)k
or the Gaussian kernel, SVMs were able to obtain extremely good perfor-
mance on this problem. This was particularly surprising since the input
attributes x were just 256-dimensional vectors of the image pixel intensity
values, and the system had no prior knowledge about vision, or even about
which pixels are adjacent to which other ones. Another example that we
brieﬂy talked about in lecture was that if the objects x that we are trying
to classify are strings (say, x is a list of amino acids, which strung together
form a protein), then it seems hard to construct a reasonable, “small” set of
features for most learning algorithms, especially if diﬀerent strings have dif-
ferent lengths. However, consider letting φ(x) be a feature vector that counts
the number of occurrences of each length-k substring in x. If we’re consid-
ering strings of English letters, then there are 26k such strings. Hence, φ(x)
is a 26k dimensional vector; even for moderate values of k, this is probably
too big for us to eﬃciently work with. (e.g., 264 ≈ 460000.) However, using
(dynamic programming-ish) string matching algorithms, it is possible to ef-
ﬁciently compute K(x, z) = φ(x)T φ(z), so that we can now implicitly work
in this 26k-dimensional feature space, but without ever explicitly computing
feature vectors in this space.

3Many texts present Mercer’s theorem in a slightly more complicated form involving
L2 functions, but when the input attributes take values in Rd, the version given here is
equivalent.

11

Application of kernel methods: We’ve seen the application of kernels
to linear regression. In the next part, we will introduce the support vector
machines to which kernels can be directly applied. dwell too much longer on
it here. In fact, the idea of kernels has signiﬁcantly broader applicability than
linear regression and SVMs. Speciﬁcally, if you have any learning algorithm
that you can write in terms of only inner products (cid:104)x, z(cid:105) between input
attribute vectors, then by replacing this with K(x, z) where K is a kernel,
you can “magically” allow your algorithm to work eﬃciently in the high
dimensional feature space corresponding to K. For instance, this kernel trick
can be applied with the perceptron to derive a kernel perceptron algorithm.
Many of the algorithms that we’ll see later in this class will also be amenable
to this method, which has come to be known as the “kernel trick.”

Part VI
Support Vector Machines

This set of notes presents the Support Vector Machine (SVM) learning al-
gorithm. SVMs are among the best (and many believe are indeed the best)
“oﬀ-the-shelf” supervised learning algorithms. To tell the SVM story, we’ll
need to ﬁrst talk about margins and the idea of separating data with a large
“gap.” Next, we’ll talk about the optimal margin classiﬁer, which will lead
us into a digression on Lagrange duality. We’ll also see kernels, which give
a way to apply SVMs eﬃciently in very high dimensional (such as inﬁnite-
dimensional) feature spaces, and ﬁnally, we’ll close oﬀ the story with the
SMO algorithm, which gives an eﬃcient implementation of SVMs.

2 Margins: Intuition

We’ll start our story on SVMs by talking about margins. This section will
give the intuitions about margins and about the “conﬁdence” of our predic-
tions; these ideas will be made formal in Section 4.

Consider logistic regression, where the probability p(y = 1|x; θ) is mod-
eled by hθ(x) = g(θT x). We then predict “1” on an input x if and only if
hθ(x) ≥ 0.5, or equivalently, if and only if θT x ≥ 0. Consider a positive
training example (y = 1). The larger θT x is, the larger also is hθ(x) = p(y =
1|x; θ), and thus also the higher our degree of “conﬁdence” that the label is 1.
Thus, informally we can think of our prediction as being very conﬁdent that

12

y = 1 if θT x (cid:29) 0. Similarly, we think of logistic regression as conﬁdently
predicting y = 0, if θT x (cid:28) 0. Given a training set, again informally it seems
that we’d have found a good ﬁt to the training data if we can ﬁnd θ so that
θT x(i) (cid:29) 0 whenever y(i) = 1, and θT x(i) (cid:28) 0 whenever y(i) = 0, since this
would reﬂect a very conﬁdent (and correct) set of classiﬁcations for all the
training examples. This seems to be a nice goal to aim for, and we’ll soon
formalize this idea using the notion of functional margins.

For a diﬀerent type of intuition, consider the following ﬁgure, in which x’s
represent positive training examples, o’s denote negative training examples,
a decision boundary (this is the line given by the equation θT x = 0, and
is also called the separating hyperplane) is also shown, and three points
have also been labeled A, B and C.

Notice that the point A is very far from the decision boundary. If we are
asked to make a prediction for the value of y at A, it seems we should be
quite conﬁdent that y = 1 there. Conversely, the point C is very close to
the decision boundary, and while it’s on the side of the decision boundary
on which we would predict y = 1, it seems likely that just a small change to
the decision boundary could easily have caused out prediction to be y = 0.
Hence, we’re much more conﬁdent about our prediction at A than at C. The
point B lies in-between these two cases, and more broadly, we see that if
a point is far from the separating hyperplane, then we may be signiﬁcantly
more conﬁdent in our predictions. Again, informally we think it would be
nice if, given a training set, we manage to ﬁnd a decision boundary that
allows us to make all correct and conﬁdent (meaning far from the decision
boundary) predictions on the training examples. We’ll formalize this later
using the notion of geometric margins.

(cid:0)(cid:1)(cid:0)(cid:1)(cid:0)(cid:1)BAC13

3 Notation

To make our discussion of SVMs easier, we’ll ﬁrst need to introduce a new
notation for talking about classiﬁcation. We will be considering a linear
classiﬁer for a binary classiﬁcation problem with labels y and features x.
From now, we’ll use y ∈ {−1, 1} (instead of {0, 1}) to denote the class labels.
Also, rather than parameterizing our linear classiﬁer with the vector θ, we
will use parameters w, b, and write our classiﬁer as

hw,b(x) = g(wT x + b).

Here, g(z) = 1 if z ≥ 0, and g(z) = −1 otherwise. This “w, b” notation
allows us to explicitly treat the intercept term b separately from the other
parameters. (We also drop the convention we had previously of letting x0 = 1
be an extra coordinate in the input feature vector.) Thus, b takes the role of
what was previously θ0, and w takes the role of [θ1 . . . θd]T .

Note also that, from our deﬁnition of g above, our classiﬁer will directly
predict either 1 or −1 (cf. the perceptron algorithm), without ﬁrst going
through the intermediate step of estimating p(y = 1) (which is what logistic
regression does).

4 Functional and geometric margins

Let’s formalize the notions of the functional and geometric margins. Given a
training example (x(i), y(i)), we deﬁne the functional margin of (w, b) with
respect to the training example as

ˆγ(i) = y(i)(wT x(i) + b).

Note that if y(i) = 1, then for the functional margin to be large (i.e., for
our prediction to be conﬁdent and correct), we need wT x(i) + b to be a large
positive number. Conversely, if y(i) = −1, then for the functional margin
to be large, we need wT x(i) + b to be a large negative number. Moreover, if
y(i)(wT x(i) + b) > 0, then our prediction on this example is correct. (Check
this yourself.) Hence, a large functional margin represents a conﬁdent and a
correct prediction.

For a linear classiﬁer with the choice of g given above (taking values in
{−1, 1}), there’s one property of the functional margin that makes it not a
very good measure of conﬁdence, however. Given our choice of g, we note that
if we replace w with 2w and b with 2b, then since g(wT x + b) = g(2wT x + 2b),

14

this would not change hw,b(x) at all. I.e., g, and hence also hw,b(x), depends
only on the sign, but not on the magnitude, of wT x + b. However, replacing
(w, b) with (2w, 2b) also results in multiplying our functional margin by a
factor of 2. Thus, it seems that by exploiting our freedom to scale w and b,
we can make the functional margin arbitrarily large without really changing
anything meaningful. Intuitively, it might therefore make sense to impose
some sort of normalization condition such as that ||w||2 = 1; i.e., we might
replace (w, b) with (w/||w||2, b/||w||2), and instead consider the functional
margin of (w/||w||2, b/||w||2). We’ll come back to this later.

Given a training set S = {(x(i), y(i)); i = 1, . . . , n}, we also deﬁne the
function margin of (w, b) with respect to S as the smallest of the functional
margins of the individual training examples. Denoted by ˆγ, this can therefore
be written:

ˆγ = min
i=1,...,n

ˆγ(i).

Next, let’s talk about geometric margins. Consider the picture below:

The decision boundary corresponding to (w, b) is shown, along with the
vector w. Note that w is orthogonal (at 90◦) to the separating hyperplane.
(You should convince yourself that this must be the case.) Consider the
point at A, which represents the input x(i) of some training example with
label y(i) = 1. Its distance to the decision boundary, γ(i), is given by the line
segment AB.

How can we ﬁnd the value of γ(i)? Well, w/||w|| is a unit-length vector
pointing in the same direction as w. Since A represents x(i), we therefore
ﬁnd that the point B is given by x(i) − γ(i) · w/||w||. But this point lies on

wAγB(i)15

the decision boundary, and all points x on the decision boundary satisfy the
equation wT x + b = 0. Hence,

wT

(cid:18)
x(i) − γ(i) w
||w||

(cid:19)

+ b = 0.

Solving for γ(i) yields

γ(i) =

wT x(i) + b
||w||

=

(cid:18) w
||w||

(cid:19)T

x(i) +

b
||w||

.

This was worked out for the case of a positive training example at A in the
ﬁgure, where being on the “positive” side of the decision boundary is good.
More generally, we deﬁne the geometric margin of (w, b) with respect to a
training example (x(i), y(i)) to be

γ(i) = y(i)

(cid:19)T

(cid:32)(cid:18) w
||w||

x(i) +

(cid:33)

.

b
||w||

Note that if ||w|| = 1, then the functional margin equals the geometric
margin—this thus gives us a way of relating these two diﬀerent notions of
margin. Also, the geometric margin is invariant to rescaling of the parame-
ters; i.e., if we replace w with 2w and b with 2b, then the geometric margin
does not change. This will in fact come in handy later. Speciﬁcally, because
of this invariance to the scaling of the parameters, when trying to ﬁt w and b
to training data, we can impose an arbitrary scaling constraint on w without
changing anything important; for instance, we can demand that ||w|| = 1, or
|w1| = 5, or |w1 + b| + |w2| = 2, and any of these can be satisﬁed simply by
rescaling w and b.

Finally, given a training set S = {(x(i), y(i)); i = 1, . . . , n}, we also deﬁne
the geometric margin of (w, b) with respect to S to be the smallest of the
geometric margins on the individual training examples:

γ = min
i=1,...,n

γ(i).

5 The optimal margin classiﬁer

Given a training set, it seems from our previous discussion that a natural
desideratum is to try to ﬁnd a decision boundary that maximizes the (ge-
ometric) margin, since this would reﬂect a very conﬁdent set of predictions

16

on the training set and a good “ﬁt” to the training data. Speciﬁcally, this
will result in a classiﬁer that separates the positive and the negative training
examples with a “gap” (geometric margin).

For now, we will assume that we are given a training set that is linearly
separable; i.e., that it is possible to separate the positive and negative ex-
amples using some separating hyperplane. How will we ﬁnd the one that
achieves the maximum geometric margin? We can pose the following opti-
mization problem:

maxγ,w,b γ

s.t. y(i)(wT x(i) + b) ≥ γ,

i = 1, . . . , n

||w|| = 1.

I.e., we want to maximize γ, subject to each training example having func-
tional margin at least γ. The ||w|| = 1 constraint moreover ensures that the
functional margin equals to the geometric margin, so we are also guaranteed
that all the geometric margins are at least γ. Thus, solving this problem will
result in (w, b) with the largest possible geometric margin with respect to the
training set.

If we could solve the optimization problem above, we’d be done. But the
“||w|| = 1” constraint is a nasty (non-convex) one, and this problem certainly
isn’t in any format that we can plug into standard optimization software to
solve. So, let’s try transforming the problem into a nicer one. Consider:

maxˆγ,w,b

ˆγ
||w||

s.t. y(i)(wT x(i) + b) ≥ ˆγ,

i = 1, . . . , n

Here, we’re going to maximize ˆγ/||w||, subject to the functional margins all
being at least ˆγ. Since the geometric and functional margins are related by
γ = ˆγ/||w|, this will give us the answer we want. Moreover, we’ve gotten rid
of the constraint ||w|| = 1 that we didn’t like. The downside is that we now
ˆγ
have a nasty (again, non-convex) objective
||w|| function; and, we still don’t
have any oﬀ-the-shelf software that can solve this form of an optimization
problem.

Let’s keep going. Recall our earlier discussion that we can add an arbi-
trary scaling constraint on w and b without changing anything. This is the
key idea we’ll use now. We will introduce the scaling constraint that the
functional margin of w, b with respect to the training set must be 1:

ˆγ = 1.

17

Since multiplying w and b by some constant results in the functional margin
being multiplied by that same constant, this is indeed a scaling constraint,
and can be satisﬁed by rescaling w, b. Plugging this into our problem above,
and noting that maximizing ˆγ/||w|| = 1/||w|| is the same thing as minimizing
||w||2, we now have the following optimization problem:

minw,b

1
2

||w||2

s.t. y(i)(wT x(i) + b) ≥ 1,

i = 1, . . . , n

We’ve now transformed the problem into a form that can be eﬃciently
solved. The above is an optimization problem with a convex quadratic ob-
jective and only linear constraints. Its solution gives us the optimal mar-
gin classiﬁer. This optimization problem can be solved using commercial
quadratic programming (QP) code.4

While we could call the problem solved here, what we will instead do is
make a digression to talk about Lagrange duality. This will lead us to our
optimization problem’s dual form, which will play a key role in allowing us to
use kernels to get optimal margin classiﬁers to work eﬃciently in very high
dimensional spaces. The dual form will also allow us to derive an eﬃcient
algorithm for solving the above optimization problem that will typically do
much better than generic QP software.

6 Lagrange duality (optional reading)

Let’s temporarily put aside SVMs and maximum margin classiﬁers, and talk
about solving constrained optimization problems.
Consider a problem of the following form:

minw

f (w)
s.t. hi(w) = 0,

i = 1, . . . , l.

Some of you may recall how the method of Lagrange multipliers can be used
to solve it. (Don’t worry if you haven’t seen it before.) In this method, we
deﬁne the Lagrangian to be

L(w, β) = f (w) +

l
(cid:88)

i=1

βihi(w)

4You may be familiar with linear programming, which solves optimization problems
that have linear objectives and linear constraints. QP software is also widely available,
which allows convex quadratic objectives and linear constraints.

18

Here, the βi’s are called the Lagrange multipliers. We would then ﬁnd
and set L’s partial derivatives to zero:

∂L
∂wi

= 0;

∂L
∂βi

= 0,

and solve for w and β.

In this section, we will generalize this to constrained optimization prob-
lems in which we may have inequality as well as equality constraints. Due to
time constraints, we won’t really be able to do the theory of Lagrange duality
justice in this class,5 but we will give the main ideas and results, which we
will then apply to our optimal margin classiﬁer’s optimization problem.

Consider the following, which we’ll call the primal optimization problem:

minw

f (w)
s.t. gi(w) ≤ 0,
hi(w) = 0,

i = 1, . . . , k
i = 1, . . . , l.

To solve it, we start by deﬁning the generalized Lagrangian

L(w, α, β) = f (w) +

k
(cid:88)

i=1

αigi(w) +

l
(cid:88)

i=1

βihi(w).

Here, the αi’s and βi’s are the Lagrange multipliers. Consider the quantity

θP(w) = max

α,β : αi≥0

L(w, α, β).

Here, the “P” subscript stands for “primal.” Let some w be given.
If w
violates any of the primal constraints (i.e., if either gi(w) > 0 or hi(w) (cid:54)= 0
for some i), then you should be able to verify that

θP(w) = max

α,β : αi≥0

f (w) +

= ∞.

k
(cid:88)

i=1

αigi(w) +

l
(cid:88)

i=1

βihi(w)

(13)

(14)

Conversely, if the constraints are indeed satisﬁed for a particular value of w,
then θP(w) = f (w). Hence,

θP(w) =

(cid:26) f (w)

if w satisﬁes primal constraints

∞ otherwise.

5Readers interested in learning more about this topic are encouraged to read, e.g., R.

T. Rockarfeller (1970), Convex Analysis, Princeton University Press.

19

Thus, θP takes the same value as the objective in our problem for all val-
ues of w that satisﬁes the primal constraints, and is positive inﬁnity if the
constraints are violated. Hence, if we consider the minimization problem

min
w

θP(w) = min

w

max
α,β : αi≥0

L(w, α, β),

we see that it is the same problem (i.e., and has the same solutions as) our
original, primal problem. For later use, we also deﬁne the optimal value of
the objective to be p∗ = minw θP(w); we call this the value of the primal
problem.

Now, let’s look at a slightly diﬀerent problem. We deﬁne

θD(α, β) = min

L(w, α, β).

w
Here, the “D” subscript stands for “dual.” Note also that whereas in the
deﬁnition of θP we were optimizing (maximizing) with respect to α, β, here
we are minimizing with respect to w.

We can now pose the dual optimization problem:

max
α,β : αi≥0

θD(α, β) = max

α,β : αi≥0

min
w

L(w, α, β).

This is exactly the same as our primal problem shown above, except that the
order of the “max” and the “min” are now exchanged. We also deﬁne the
optimal value of the dual problem’s objective to be d∗ = maxα,β : αi≥0 θD(w).
How are the primal and the dual problems related? It can easily be shown

that

d∗ = max

α,β : αi≥0

min
w

L(w, α, β) ≤ min

w

max
α,β : αi≥0

L(w, α, β) = p∗.

(You should convince yourself of this; this follows from the “max min” of a
function always being less than or equal to the “min max.”) However, under
certain conditions, we will have

d∗ = p∗,

so that we can solve the dual problem in lieu of the primal problem. Let’s
see what these conditions are.

Suppose f and the gi’s are convex,6 and the hi’s are aﬃne.7 Suppose
further that the constraints gi are (strictly) feasible; this means that there
exists some w so that gi(w) < 0 for all i.

6When f has a Hessian, then it is convex if and only if the Hessian is positive semi-
deﬁnite. For instance, f (w) = wT w is convex; similarly, all linear (and aﬃne) functions
are also convex. (A function f can also be convex without being diﬀerentiable, but we
won’t need those more general deﬁnitions of convexity here.)

7I.e., there exists ai, bi, so that hi(w) = aT

i w + bi. “Aﬃne” means the same thing as

linear, except that we also allow the extra intercept term bi.

Under our above assumptions, there must exist w∗, α∗, β∗ so that w∗ is the
solution to the primal problem, α∗, β∗ are the solution to the dual problem,
and moreover p∗ = d∗ = L(w∗, α∗, β∗). Moreover, w∗, α∗ and β∗ satisfy the
Karush-Kuhn-Tucker (KKT) conditions, which are as follows:

20

∂
∂wi
∂
∂βi

L(w∗, α∗, β∗) = 0,

i = 1, . . . , d

L(w∗, α∗, β∗) = 0,

i = 1, . . . , l

i gi(w∗) = 0,
α∗
gi(w∗) ≤ 0,
α∗ ≥ 0,

i = 1, . . . , k
i = 1, . . . , k
i = 1, . . . , k

(15)

(16)

(17)
(18)
(19)

Moreover, if some w∗, α∗, β∗ satisfy the KKT conditions, then it is also a
solution to the primal and dual problems.

We draw attention to Equation (17), which is called the KKT dual
complementarity condition. Speciﬁcally, it implies that if α∗
i > 0, then
gi(w∗) = 0. (I.e., the “gi(w) ≤ 0” constraint is active, meaning it holds with
equality rather than with inequality.) Later on, this will be key for showing
that the SVM has only a small number of “support vectors”; the KKT dual
complementarity condition will also give us our convergence test when we
talk about the SMO algorithm.

7 Optimal margin classiﬁers

Note: The equivalence of optimization problem (20) and the optimization
problem (24), and the relationship between the primary and dual variables in
equation (22) are the most important take home messages of this section.

Previously, we posed the following (primal) optimization problem for ﬁnd-

ing the optimal margin classiﬁer:

minw,b

1
2

||w||2

s.t. y(i)(wT x(i) + b) ≥ 1,

i = 1, . . . , n

(20)

We can write the constraints as

gi(w) = −y(i)(wT x(i) + b) + 1 ≤ 0.

21

We have one such constraint for each training example. Note that from the
KKT dual complementarity condition, we will have αi > 0 only for the train-
ing examples that have functional margin exactly equal to one (i.e., the ones
corresponding to constraints that hold with equality, gi(w) = 0). Consider
the ﬁgure below, in which a maximum margin separating hyperplane is shown
by the solid line.

The points with the smallest margins are exactly the ones closest to the
decision boundary; here, these are the three points (one negative and two pos-
itive examples) that lie on the dashed lines parallel to the decision boundary.
Thus, only three of the αi’s—namely, the ones corresponding to these three
training examples—will be non-zero at the optimal solution to our optimiza-
tion problem. These three points are called the support vectors in this
problem. The fact that the number of support vectors can be much smaller
than the size the training set will be useful later.

Let’s move on. Looking ahead, as we develop the dual form of the prob-
lem, one key idea to watch out for is that we’ll try to write our algorithm
in terms of only the inner product (cid:104)x(i), x(j)(cid:105) (think of this as (x(i))T x(j))
between points in the input feature space. The fact that we can express our
algorithm in terms of these inner products will be key when we apply the
kernel trick.

When we construct the Lagrangian for our optimization problem we have:

L(w, b, α) =

1
2

||w||2 −

n
(cid:88)

i=1

(cid:2)y(i)(wT x(i) + b) − 1(cid:3) .

αi

(21)

Note that there’re only “αi” but no “βi” Lagrange multipliers, since the
problem has only inequality constraints.

22

Let’s ﬁnd the dual form of the problem. To do so, we need to ﬁrst
minimize L(w, b, α) with respect to w and b (for ﬁxed α), to get θD, which
we’ll do by setting the derivatives of L with respect to w and b to zero. We
have:

∇wL(w, b, α) = w −

αiy(i)x(i) = 0

n
(cid:88)

This implies that

i=1

w =

n
(cid:88)

i=1

αiy(i)x(i).

As for the derivative with respect to b, we obtain

∂
∂b

L(w, b, α) =

n
(cid:88)

i=1

αiy(i) = 0.

(22)

(23)

If we take the deﬁnition of w in Equation (22) and plug that back into

the Lagrangian (Equation 21), and simplify, we get

L(w, b, α) =

n
(cid:88)

i=1

αi −

1
2

n
(cid:88)

i,j=1

y(i)y(j)αiαj(x(i))T x(j) − b

n
(cid:88)

i=1

αiy(i).

But from Equation (23), the last term must be zero, so we obtain

L(w, b, α) =

n
(cid:88)

i=1

αi −

1
2

n
(cid:88)

i,j=1

y(i)y(j)αiαj(x(i))T x(j).

Recall that we got to the equation above by minimizing L with respect to w
and b. Putting this together with the constraints αi ≥ 0 (that we always had)
and the constraint (23), we obtain the following dual optimization problem:

maxα W (α) =

n
(cid:88)

αi −

n
(cid:88)

y(i)y(j)αiαj(cid:104)x(i), x(j)(cid:105).

(24)

1
2

i=1
i = 1, . . . , n

i,j=1

s.t. αi ≥ 0,
n
(cid:88)

αiy(i) = 0,

i=1

You should also be able to verify that the conditions required for p∗ = d∗
and the KKT conditions (Equations 15–19) to hold are indeed satisﬁed in
our optimization problem. Hence, we can solve the dual in lieu of solving

23

the primal problem. Speciﬁcally, in the dual problem above, we have a
maximization problem in which the parameters are the αi’s. We’ll talk later
about the speciﬁc algorithm that we’re going to use to solve the dual problem,
but if we are indeed able to solve it (i.e., ﬁnd the α’s that maximize W (α)
subject to the constraints), then we can use Equation (22) to go back and
ﬁnd the optimal w’s as a function of the α’s. Having found w∗, by considering
the primal problem, it is also straightforward to ﬁnd the optimal value for
the intercept term b as

b∗ = −

maxi:y(i)=−1 w∗T x(i) + mini:y(i)=1 w∗T x(i)
2

.

(25)

(Check for yourself that this is correct.)

Before moving on, let’s also take a more careful look at Equation (22),
which gives the optimal value of w in terms of (the optimal value of) α.
Suppose we’ve ﬁt our model’s parameters to a training set, and now wish to
make a prediction at a new point input x. We would then calculate wT x + b,
and predict y = 1 if and only if this quantity is bigger than zero. But
using (22), this quantity can also be written:

wT x + b =

(cid:33)T

αiy(i)x(i)

x + b

(cid:32) n

(cid:88)

i=1

=

n
(cid:88)

i=1

αiy(i)(cid:104)x(i), x(cid:105) + b.

(26)

(27)

Hence, if we’ve found the αi’s, in order to make a prediction, we have to
calculate a quantity that depends only on the inner product between x and
the points in the training set. Moreover, we saw earlier that the αi’s will all
be zero except for the support vectors. Thus, many of the terms in the sum
above will be zero, and we really need to ﬁnd only the inner products between
x and the support vectors (of which there is often only a small number) in
order calculate (27) and make our prediction.

By examining the dual form of the optimization problem, we gained sig-
niﬁcant insight into the structure of the problem, and were also able to write
the entire algorithm in terms of only inner products between input feature
vectors. In the next section, we will exploit this property to apply the ker-
nels to our classiﬁcation problem. The resulting algorithm, support vector
machines, will be able to eﬃciently learn in very high dimensional spaces.

24

8 Regularization and the non-separable case

(optional reading)

The derivation of the SVM as presented so far assumed that the data is
linearly separable. While mapping data to a high dimensional feature space
via φ does generally increase the likelihood that the data is separable, we
can’t guarantee that it always will be so. Also, in some cases it is not clear
that ﬁnding a separating hyperplane is exactly what we’d want to do, since
that might be susceptible to outliers. For instance, the left ﬁgure below
shows an optimal margin classiﬁer, and when a single outlier is added in the
upper-left region (right ﬁgure), it causes the decision boundary to make a
dramatic swing, and the resulting classiﬁer has a much smaller margin.

To make the algorithm work for non-linearly separable datasets as well
as be less sensitive to outliers, we reformulate our optimization (using (cid:96)1
regularization) as follows:

minγ,w,b

1
2

||w||2 + C

n
(cid:88)

i=1

ξi

s.t. y(i)(wT x(i) + b) ≥ 1 − ξi,

i = 1, . . . , n

ξi ≥ 0,

i = 1, . . . , n.

Thus, examples are now permitted to have (functional) margin less than 1,
and if an example has functional margin 1 − ξi (with ξ > 0), we would pay
a cost of the objective function being increased by Cξi. The parameter C

25

controls the relative weighting between the twin goals of making the ||w||2
small (which we saw earlier makes the margin large) and of ensuring that
most examples have functional margin at least 1.

As before, we can form the Lagrangian:

L(w, b, ξ, α, r) =

1
2

wT w + C

n
(cid:88)

n
(cid:88)

ξi −

i=1

i=1

(cid:2)y(i)(xT w + b) − 1 + ξi

(cid:3) −

αi

n
(cid:88)

i=1

riξi.

Here, the αi’s and ri’s are our Lagrange multipliers (constrained to be ≥ 0).
We won’t go through the derivation of the dual again in detail, but after
setting the derivatives with respect to w and b to zero as before, substituting
them back in, and simplifying, we obtain the following dual form of the
problem:

maxα W (α) =

n
(cid:88)

αi −

1
2

n
(cid:88)

i,j=1

y(i)y(j)αiαj(cid:104)x(i), x(j)(cid:105)

i=1
s.t. 0 ≤ αi ≤ C,

i = 1, . . . , n

n
(cid:88)

i=1

αiy(i) = 0,

As before, we also have that w can be expressed in terms of the αi’s
as given in Equation (22), so that after solving the dual problem, we can
continue to use Equation (27) to make our predictions. Note that, somewhat
surprisingly, in adding (cid:96)1 regularization, the only change to the dual problem
is that what was originally a constraint that 0 ≤ αi has now become 0 ≤
αi ≤ C. The calculation for b∗ also has to be modiﬁed (Equation 25 is no
longer valid); see the comments in the next section/Platt’s paper.

Also, the KKT dual-complementarity conditions (which in the next sec-
tion will be useful for testing for the convergence of the SMO algorithm)
are:

αi = 0 ⇒ y(i)(wT x(i) + b) ≥ 1
αi = C ⇒ y(i)(wT x(i) + b) ≤ 1
0 < αi < C ⇒ y(i)(wT x(i) + b) = 1.

(28)
(29)
(30)

Now, all that remains is to give an algorithm for actually solving the dual

problem, which we will do in the next section.

26

9 The SMO algorithm (optional reading)

The SMO (sequential minimal optimization) algorithm, due to John Platt,
gives an eﬃcient way of solving the dual problem arising from the derivation
of the SVM. Partly to motivate the SMO algorithm, and partly because it’s
interesting in its own right, let’s ﬁrst take another digression to talk about
the coordinate ascent algorithm.

9.1 Coordinate ascent

Consider trying to solve the unconstrained optimization problem

max
α

W (α1, α2, . . . , αn).

Here, we think of W as just some function of the parameters αi’s, and for now
ignore any relationship between this problem and SVMs. We’ve already seen
two optimization algorithms, gradient ascent and Newton’s method. The
new algorithm we’re going to consider here is called coordinate ascent:

Loop until convergence: {

For i = 1, . . . , n, {

αi := arg max ˆαi W (α1, . . . , αi−1, ˆαi, αi+1, . . . , αn).

}

}

Thus, in the innermost loop of this algorithm, we will hold all the variables
except for some αi ﬁxed, and reoptimize W with respect to just the parameter
αi. In the version of this method presented here, the inner-loop reoptimizes
the variables in order α1, α2, . . . , αn, α1, α2, . . .. (A more sophisticated version
might choose other orderings; for instance, we may choose the next variable
to update according to which one we expect to allow us to make the largest
increase in W (α).)

When the function W happens to be of such a form that the “arg max”
in the inner loop can be performed eﬃciently, then coordinate ascent can be
a fairly eﬃcient algorithm. Here’s a picture of coordinate ascent in action:

27

The ellipses in the ﬁgure are the contours of a quadratic function that
we want to optimize. Coordinate ascent was initialized at (2, −2), and also
plotted in the ﬁgure is the path that it took on its way to the global maximum.
Notice that on each step, coordinate ascent takes a step that’s parallel to one
of the axes, since only one variable is being optimized at a time.

9.2 SMO

We close oﬀ the discussion of SVMs by sketching the derivation of the SMO
algorithm. Some details will be left to the homework, and for others you
may refer to the paper excerpt handed out in class.

Here’s the (dual) optimization problem that we want to solve:

maxα W (α) =

n
(cid:88)

αi −

1
2

n
(cid:88)

i,j=1

y(i)y(j)αiαj(cid:104)x(i), x(j)(cid:105).

i=1
s.t. 0 ≤ αi ≤ C,

i = 1, . . . , n

n
(cid:88)

i=1

αiy(i) = 0.

(31)

(32)

(33)

Let’s say we have set of αi’s that satisfy the constraints (32-33). Now,
suppose we want to hold α2, . . . , αn ﬁxed, and take a coordinate ascent step
and reoptimize the objective with respect to α1. Can we make any progress?
The answer is no, because the constraint (33) ensures that

α1y(1) = −

n
(cid:88)

i=2

αiy(i).

−2−1.5−1−0.500.511.522.5−2−1.5−1−0.500.511.522.528

Or, by multiplying both sides by y(1), we equivalently have

α1 = −y(1)

n
(cid:88)

i=2

αiy(i).

(This step used the fact that y(1) ∈ {−1, 1}, and hence (y(1))2 = 1.) Hence,
α1 is exactly determined by the other αi’s, and if we were to hold α2, . . . , αn
ﬁxed, then we can’t make any change to α1 without violating the con-
straint (33) in the optimization problem.

Thus, if we want to update some subject of the αi’s, we must update at
least two of them simultaneously in order to keep satisfying the constraints.
This motivates the SMO algorithm, which simply does the following:

Repeat till convergence {

1. Select some pair αi and αj to update next (using a heuristic that
tries to pick the two that will allow us to make the biggest progress
towards the global maximum).

2. Reoptimize W (α) with respect to αi and αj, while holding all the

other αk’s (k (cid:54)= i, j) ﬁxed.

}

To test for convergence of this algorithm, we can check whether the KKT
conditions (Equations 28-30) are satisﬁed to within some tol. Here, tol is
the convergence tolerance parameter, and is typically set to around 0.01 to
0.001. (See the paper and pseudocode for details.)

The key reason that SMO is an eﬃcient algorithm is that the update to
αi, αj can be computed very eﬃciently. Let’s now brieﬂy sketch the main
ideas for deriving the eﬃcient update.

Let’s say we currently have some setting of the αi’s that satisfy the con-
straints (32-33), and suppose we’ve decided to hold α3, . . . , αn ﬁxed, and
want to reoptimize W (α1, α2, . . . , αn) with respect to α1 and α2 (subject to
the constraints). From (33), we require that

α1y(1) + α2y(2) = −

n
(cid:88)

i=3

αiy(i).

Since the right hand side is ﬁxed (as we’ve ﬁxed α3, . . . αn), we can just let
it be denoted by some constant ζ:

α1y(1) + α2y(2) = ζ.

(34)

We can thus picture the constraints on α1 and α2 as follows:

29

From the constraints (32), we know that α1 and α2 must lie within the box
[0, C] × [0, C] shown. Also plotted is the line α1y(1) + α2y(2) = ζ, on which we
know α1 and α2 must lie. Note also that, from these constraints, we know
L ≤ α2 ≤ H; otherwise, (α1, α2) can’t simultaneously satisfy both the box
and the straight line constraint. In this example, L = 0. But depending on
what the line α1y(1) + α2y(2) = ζ looks like, this won’t always necessarily be
the case; but more generally, there will be some lower-bound L and some
upper-bound H on the permissible values for α2 that will ensure that α1, α2
lie within the box [0, C] × [0, C].

Using Equation (34), we can also write α1 as a function of α2:

α1 = (ζ − α2y(2))y(1).

(Check this derivation yourself; we again used the fact that y(1) ∈ {−1, 1} so
that (y(1))2 = 1.) Hence, the objective W (α) can be written

W (α1, α2, . . . , αn) = W ((ζ − α2y(2))y(1), α2, . . . , αn).

Treating α3, . . . , αn as constants, you should be able to verify that this is
just some quadratic function in α2. I.e., this can also be expressed in the
form aα2
2 + bα2 + c for some appropriate a, b, and c. If we ignore the “box”
constraints (32) (or, equivalently, that L ≤ α2 ≤ H), then we can easily
maximize this quadratic function by setting its derivative to zero and solving.
We’ll let αnew,unclipped
denote the resulting value of α2. You should also be
2
able to convince yourself that if we had instead wanted to maximize W with
respect to α2 but subject to the box constraint, then we can ﬁnd the resulting
value optimal simply by taking αnew,unclipped
and “clipping” it to lie in the

2

α2α1α1α2CC(1)+(2)yy=ζHL[L, H] interval, to get

αnew
2

=






H
αnew,unclipped
2
L

2

if αnew,unclipped
> H
if L ≤ αnew,unclipped
2
if αnew,unclipped
< L

2

30

≤ H

Finally, having found the αnew
the optimal value of αnew

.

2

1

, we can use Equation (34) to go back and ﬁnd

There’re a couple more details that are quite easy but that we’ll leave you
to read about yourself in Platt’s paper: One is the choice of the heuristics
used to select the next αi, αj to update; the other is how to update b as the
SMO algorithm is run.

